{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Logistic Regression Without Smote (80, 10, 10)"
      ],
      "metadata": {
        "id": "IdGBDYPyu9Ke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op1hPjiLu3Y0",
        "outputId": "47cc06e8-3b2c-4090-8e9a-f964289c3e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__solver': 'lbfgs'}\n",
            "Validation set accuracy: 0.58\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.60      0.80      0.69      1580\n",
            "        True       0.54      0.30      0.39      1218\n",
            "\n",
            "    accuracy                           0.58      2798\n",
            "   macro avg       0.57      0.55      0.54      2798\n",
            "weighted avg       0.57      0.58      0.56      2798\n",
            "\n",
            "Testing set accuracy: 0.55\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.56      0.78      0.65       378\n",
            "        True       0.51      0.27      0.36       322\n",
            "\n",
            "    accuracy                           0.55       700\n",
            "   macro avg       0.53      0.53      0.50       700\n",
            "weighted avg       0.54      0.55      0.51       700\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Split the data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Define the pipeline with preprocessing and LogisticRegression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(random_state=0))\n",
        "])\n",
        "\n",
        "# Define a simplified hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
        "    'classifier__solver': ['liblinear', 'lbfgs']  # Optimization algorithm\n",
        "}\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation (reduced from 5)\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1 Logistic Regression With Smote (80,10,10)"
      ],
      "metadata": {
        "id": "nKVYZOOgvH38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=0)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_smote, y_smote, test_size=0.5, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Print the sizes of the training, validation, and testing sets\n",
        "print(f'Training set size: {X_train.shape[0]} samples')\n",
        "print(f'Validation set size: {X_val.shape[0]} samples')\n",
        "print(f'Testing set size: {X_test.shape[0]} samples')\n",
        "\n",
        "# Define the pipeline with preprocessing and LogisticRegression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(random_state=0))\n",
        "])\n",
        "\n",
        "# Define a simplified hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1.0, 10.0],  # Regularization parameter\n",
        "    'classifier__solver': ['liblinear', 'lbfgs']  # Optimization algorithm\n",
        "}\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation (reduced from 5)\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul9urSlAvHQC",
        "outputId": "40d5c7ff-a05a-466c-988b-81c8d0b8900c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 3891 samples\n",
            "Validation set size: 3112 samples\n",
            "Testing set size: 779 samples\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best Hyperparameters: {'classifier__C': 0.1, 'classifier__solver': 'liblinear'}\n",
            "Validation set accuracy: 0.55\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.56      0.53      0.55      1571\n",
            "        True       0.55      0.58      0.56      1541\n",
            "\n",
            "    accuracy                           0.55      3112\n",
            "   macro avg       0.55      0.55      0.55      3112\n",
            "weighted avg       0.55      0.55      0.55      3112\n",
            "\n",
            "Testing set accuracy: 0.59\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.61      0.57      0.59       407\n",
            "        True       0.56      0.60      0.58       372\n",
            "\n",
            "    accuracy                           0.59       779\n",
            "   macro avg       0.59      0.59      0.59       779\n",
            "weighted avg       0.59      0.59      0.59       779\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. KNN Without Smote (80,10,10)"
      ],
      "metadata": {
        "id": "fQCQfR5XvPs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Split the data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Print the sizes of the training, validation, and testing sets\n",
        "print(f'Training set size: {X_train.shape[0]} samples')\n",
        "print(f'Validation set size: {X_val.shape[0]} samples')\n",
        "print(f'Testing set size: {X_test.shape[0]} samples')\n",
        "\n",
        "# Define the pipeline with preprocessing and KNN classifier\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define a hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__n_neighbors': [3, 5, 7],  # Number of neighbors\n",
        "    'classifier__weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
        "}\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy for KNN: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNUN7tLDvP_Z",
        "outputId": "fce41d75-e037-4a4e-9198-34382709735d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 5596 samples\n",
            "Validation set size: 699 samples\n",
            "Testing set size: 700 samples\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best Hyperparameters: {'classifier__n_neighbors': 7, 'classifier__weights': 'distance'}\n",
            "Validation set accuracy: 0.51\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.55      0.63      0.59       384\n",
            "        True       0.45      0.37      0.40       315\n",
            "\n",
            "    accuracy                           0.51       699\n",
            "   macro avg       0.50      0.50      0.50       699\n",
            "weighted avg       0.50      0.51      0.50       699\n",
            "\n",
            "Testing set accuracy for KNN: 0.53\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.58      0.63      0.61       397\n",
            "        True       0.46      0.40      0.43       303\n",
            "\n",
            "    accuracy                           0.53       700\n",
            "   macro avg       0.52      0.52      0.52       700\n",
            "weighted avg       0.53      0.53      0.53       700\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 KNN With Smote (80,10,10)"
      ],
      "metadata": {
        "id": "8SNKkAV8wFhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status',\n",
        "                       'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=0)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the SMOTE-transformed data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_smote, y_smote, test_size=0.2, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Print the sizes of the training, validation, and testing sets\n",
        "print(f'Training set size: {X_train.shape[0]} samples')\n",
        "print(f'Validation set size: {X_val.shape[0]} samples')\n",
        "print(f'Testing set size: {X_test.shape[0]} samples')\n",
        "\n",
        "# Define the pipeline with preprocessing and KNN classifier\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define a hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__n_neighbors': [3, 5, 7],  # Number of neighbors\n",
        "    'classifier__weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
        "}\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy for KNN with SMOTE: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju4VthWKwFAt",
        "outputId": "677d786c-3249-41c5-bf50-66070b481d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 6225 samples\n",
            "Validation set size: 778 samples\n",
            "Testing set size: 779 samples\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best Hyperparameters: {'classifier__n_neighbors': 7, 'classifier__weights': 'distance'}\n",
            "Validation set accuracy: 0.56\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.60      0.50      0.54       404\n",
            "        True       0.54      0.64      0.59       374\n",
            "\n",
            "    accuracy                           0.56       778\n",
            "   macro avg       0.57      0.57      0.56       778\n",
            "weighted avg       0.57      0.56      0.56       778\n",
            "\n",
            "Testing set accuracy for KNN with SMOTE: 0.59\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.59      0.55      0.57       390\n",
            "        True       0.58      0.62      0.60       389\n",
            "\n",
            "    accuracy                           0.59       779\n",
            "   macro avg       0.59      0.59      0.58       779\n",
            "weighted avg       0.59      0.59      0.58       779\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Naive Bayes Without Smote (80,10,10)"
      ],
      "metadata": {
        "id": "3LD7ljk_wMI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Split the data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Print the sizes of the training, validation, and testing sets\n",
        "print(f'Training set size: {X_train.shape[0]} samples')\n",
        "print(f'Validation set size: {X_val.shape[0]} samples')\n",
        "print(f'Testing set size: {X_test.shape[0]} samples')\n",
        "\n",
        "# Define the pipeline with preprocessing and GaussianNB\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation (no hyperparameters to tune for GaussianNB)\n",
        "grid_search = GridSearchCV(pipeline, {}, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found (will be empty for GaussianNB)\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy for Naive Bayes: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCpfHAvKwO8A",
        "outputId": "14b82ab5-b043-47ce-d0c6-edb3c98fbadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 5596 samples\n",
            "Validation set size: 699 samples\n",
            "Testing set size: 700 samples\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Best Hyperparameters: {}\n",
            "Validation set accuracy: 0.57\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.58      0.78      0.67       384\n",
            "        True       0.54      0.31      0.39       315\n",
            "\n",
            "    accuracy                           0.57       699\n",
            "   macro avg       0.56      0.54      0.53       699\n",
            "weighted avg       0.56      0.57      0.54       699\n",
            "\n",
            "Testing set accuracy for Naive Bayes: 0.57\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.59      0.78      0.67       397\n",
            "        True       0.51      0.29      0.37       303\n",
            "\n",
            "    accuracy                           0.57       700\n",
            "   macro avg       0.55      0.54      0.52       700\n",
            "weighted avg       0.55      0.57      0.54       700\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Naive Bayes With Smote (80,10,10)"
      ],
      "metadata": {
        "id": "jxzr2wDFwRlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the CSV data into a pandas DataFrame\n",
        "data = pd.read_csv('/content/students_mental_health_survey.csv')\n",
        "\n",
        "# Remove rows with any missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Handle missing values in categorical columns by replacing empty strings with 'Unknown'\n",
        "categorical_columns = ['Course', 'Gender', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Extracurricular_Involvement', 'Residence_Type']\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_columns:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = data.drop(columns=['Depression_Score'])\n",
        "y = data['Depression_Score'] > 2  # Binary classification: 1 if Depression_Score > 2, else 0\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=0)\n",
        "X_smote, y_smote = smote.fit_resample(X, y)\n",
        "\n",
        "# Split the data into training (80%), validation (10%), and testing (10%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_smote, y_smote, test_size=0.2, random_state=0)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=0)  # 0.5 x 0.2 = 0.1\n",
        "\n",
        "# Print the sizes of the training, validation, and testing sets\n",
        "print(f'Training set size: {X_train.shape[0]} samples')\n",
        "print(f'Validation set size: {X_val.shape[0]} samples')\n",
        "print(f'Testing set size: {X_test.shape[0]} samples')\n",
        "\n",
        "# Define the pipeline with preprocessing and GaussianNB\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', GaussianNB())\n",
        "])\n",
        "\n",
        "# Perform grid search with 3-fold cross-validation (no hyperparameters to tune for GaussianNB)\n",
        "grid_search = GridSearchCV(pipeline, {}, cv=3, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found (will be empty for GaussianNB)\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions and evaluate the model on the validation set\n",
        "y_val_pred = grid_search.predict(X_val)\n",
        "accuracy_val = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation set accuracy for naive bayes: {accuracy_val:.2f}')\n",
        "print('Validation set classification report:')\n",
        "print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "# Make predictions and evaluate the model on the testing set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Testing set accuracy for naive bayes: {accuracy:.2f}')\n",
        "print('Testing set classification report:')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlJxvtMbwW-q",
        "outputId": "f9fa3654-5c70-4497-e845-6db94b0e6de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 6225 samples\n",
            "Validation set size: 778 samples\n",
            "Testing set size: 779 samples\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Best Hyperparameters: {}\n",
            "Validation set accuracy for naive bayes: 0.57\n",
            "Validation set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.61      0.49      0.54       404\n",
            "        True       0.54      0.66      0.59       374\n",
            "\n",
            "    accuracy                           0.57       778\n",
            "   macro avg       0.57      0.57      0.57       778\n",
            "weighted avg       0.58      0.57      0.57       778\n",
            "\n",
            "Testing set accuracy for naive bayes: 0.58\n",
            "Testing set classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.59      0.48      0.53       390\n",
            "        True       0.56      0.67      0.61       389\n",
            "\n",
            "    accuracy                           0.58       779\n",
            "   macro avg       0.58      0.58      0.57       779\n",
            "weighted avg       0.58      0.58      0.57       779\n",
            "\n"
          ]
        }
      ]
    }
  ]
}